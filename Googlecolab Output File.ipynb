{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoF9GjqwJKzm",
        "colab_type": "code",
        "outputId": "0d51064c-578f-4a21-f54e-c0807177eba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "# Import libraries\n",
        "#!pip install opencv-python\n",
        "import keras\n",
        "import os,cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import backend as K\n",
        "K.common.image_data_format() =='th'\n",
        "# from keras.layers import Merge\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.optimizers import SGD,RMSprop,adam\n",
        "import glob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-r6xAoVlS9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeGPivQ2J7lP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "749339a6-dc89-48b8-bef6-6c5cdc3d0bed"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3254a9bed9fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_application_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'auth' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eod0scgpWNvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1jYkSRUW9Vr",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjm-znX5W_Gp",
        "colab_type": "code",
        "outputId": "dfc331b9-f852-4d8b-dde3-ffcd9eced816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NrDfubeXxSo",
        "colab_type": "code",
        "outputId": "83819f16-e284-4539-f5e2-155505331cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "PATH = os.getcwd()\n",
        "# Define data path\n",
        "data_path = '/content/drive/My Drive/DICE Project/OCT_Dataset/OCT_Dataset'\n",
        " \n",
        "data_dir_list = os.listdir(data_path)\n",
        "\n",
        "img_rows=128\n",
        "img_cols=128\n",
        "num_channel=1\n",
        "num_epoch=2\n",
        "no_images=0\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "    img_list = os.listdir(data_path + '/' + dataset)\n",
        "    no_images = no_images+len(img_list)\n",
        "\n",
        "# Define the number of classes\n",
        "labels = np.ones((no_images,),dtype='int64')\n",
        "num_classes = 3\n",
        "label_index=0\n",
        "img_data_list=[]\n",
        "img=0\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "    img_list=os.listdir(data_path+'/'+ dataset)\n",
        "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
        "    for img in img_list:\n",
        "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img)\n",
        "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "        input_img_resize=cv2.resize(input_img,(128,128))\n",
        "        img_data_list.append(input_img_resize)\n",
        "        if dataset[0]==  'A':\n",
        "            labels[label_index]=  0\n",
        "            #print(dataset[0])\n",
        "        if dataset[0] == 'D':\n",
        "            labels[label_index] = 1\n",
        "            #print(dataset[0])\n",
        "        if dataset[0] == 'N':\n",
        "            labels[label_index] = 2\n",
        "        label_index = label_index+1\n",
        "            #print(dataset[0])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded the images of dataset-NORMAL9\n",
            "\n",
            "Loaded the images of dataset-NORMAL7\n",
            "\n",
            "Loaded the images of dataset-NORMAL2\n",
            "\n",
            "Loaded the images of dataset-NORMAL15\n",
            "\n",
            "Loaded the images of dataset-NORMAL8\n",
            "\n",
            "Loaded the images of dataset-NORMAL3\n",
            "\n",
            "Loaded the images of dataset-NORMAL6\n",
            "\n",
            "Loaded the images of dataset-NORMAL14\n",
            "\n",
            "Loaded the images of dataset-NORMAL4\n",
            "\n",
            "Loaded the images of dataset-NORMAL5\n",
            "\n",
            "Loaded the images of dataset-NORMAL10\n",
            "\n",
            "Loaded the images of dataset-NORMAL11\n",
            "\n",
            "Loaded the images of dataset-DME8\n",
            "\n",
            "Loaded the images of dataset-DME7\n",
            "\n",
            "Loaded the images of dataset-DME6\n",
            "\n",
            "Loaded the images of dataset-NORMAL13\n",
            "\n",
            "Loaded the images of dataset-NORMAL12\n",
            "\n",
            "Loaded the images of dataset-DME9\n",
            "\n",
            "Loaded the images of dataset-DME5\n",
            "\n",
            "Loaded the images of dataset-NORMAL1\n",
            "\n",
            "Loaded the images of dataset-DME10\n",
            "\n",
            "Loaded the images of dataset-DME14\n",
            "\n",
            "Loaded the images of dataset-DME1\n",
            "\n",
            "Loaded the images of dataset-DME12\n",
            "\n",
            "Loaded the images of dataset-DME2\n",
            "\n",
            "Loaded the images of dataset-DME3\n",
            "\n",
            "Loaded the images of dataset-DME4\n",
            "\n",
            "Loaded the images of dataset-DME13\n",
            "\n",
            "Loaded the images of dataset-DME15\n",
            "\n",
            "Loaded the images of dataset-DME11\n",
            "\n",
            "Loaded the images of dataset-AMD2\n",
            "\n",
            "Loaded the images of dataset-AMD5\n",
            "\n",
            "Loaded the images of dataset-AMD6\n",
            "\n",
            "Loaded the images of dataset-AMD9\n",
            "\n",
            "Loaded the images of dataset-AMD4\n",
            "\n",
            "Loaded the images of dataset-AMD15\n",
            "\n",
            "Loaded the images of dataset-AMD7\n",
            "\n",
            "Loaded the images of dataset-AMD3\n",
            "\n",
            "Loaded the images of dataset-AMD8\n",
            "\n",
            "Loaded the images of dataset-AMD14\n",
            "\n",
            "Loaded the images of dataset-AMD12\n",
            "\n",
            "Loaded the images of dataset-AMD13\n",
            "\n",
            "Loaded the images of dataset-AMD10\n",
            "\n",
            "Loaded the images of dataset-AMD11\n",
            "\n",
            "Loaded the images of dataset-AMD1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt0lE8KlYB90",
        "colab_type": "code",
        "outputId": "f1d2d3d8-1fec-426b-e7b9-b7294ed57477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32')\n",
        "img_data /= 255\n",
        "print (img_data.shape)\n",
        "\n",
        "\n",
        "# Using 'th' for the image_dim_ordering we get accuracy >=0.99 . \n",
        "# Using 'tf' for the dim order I get accuracy >= 0.9 but on more epochs\n",
        "if num_channel==1:\n",
        "    if K.common.image_dim_ordering()=='th':\n",
        "        img_data= np.expand_dims(img_data, axis=1)\n",
        "        print (img_data.shape)\n",
        "    else:\n",
        "        img_data= np.expand_dims(img_data, axis=4)\n",
        "        print (img_data.shape)\n",
        "\n",
        "else:\n",
        "    if K.image_dim_ordering()=='th':\n",
        "        img_data=np.rollaxis(img_data,3,1)\n",
        "        print (img_data.shape)\n",
        "\n",
        "\n",
        "        labels[0:722] = 0\n",
        "        labels[723:1823] = 1\n",
        "        labels[1824:3230] = 2\n",
        "        \n",
        "        \n",
        "        X_train.shape\n",
        "        \n",
        "\n",
        "        \n",
        "USE_SKLEARN_PREPROCESSING=False\n",
        "\n",
        "if USE_SKLEARN_PREPROCESSING:\n",
        "    # using sklearn for preprocessing\n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    def image_to_feature_vector(image, size=(128, 128)):\n",
        "        # resize the image to a fixed size, then flatten the image into\n",
        "        # a list of raw pixel intensities\n",
        "        return cv2.resize(image, size).flatten()\n",
        "\n",
        "    img_data_list=[]\n",
        "    for dataset in data_dir_list:\n",
        "        img_list=os.listdir(data_path+'/'+ dataset)\n",
        "        print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
        "        for img in img_list:\n",
        "            input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
        "            input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
        "            input_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
        "            img_data_list.append(input_img_flatten)\n",
        "\n",
        "    img_data = np.array(img_data_list)\n",
        "    img_data = img_data.astype('float32')\n",
        "    print (img_data.shape)\n",
        "    img_data_scaled = preprocessing.scale(img_data)\n",
        "    print (img_data_scaled.shape)\n",
        "\n",
        "    if K.image_dim_ordering()=='th':\n",
        "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
        "        print (img_data_scaled.shape)\n",
        "\n",
        "    else:\n",
        "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
        "        print (img_data_scaled.shape)\n",
        "\n",
        "\n",
        "    if K.image_dim_ordering()=='th':\n",
        "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
        "        print (img_data_scaled.shape)\n",
        "\n",
        "    else:\n",
        "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
        "        print (img_data_scaled.shape)\n",
        "\n",
        "if USE_SKLEARN_PREPROCESSING:\n",
        "    img_data=img_data_scaled\n",
        "    \n",
        "    #%%\n",
        "# labels[0:800]\n",
        "labels[1500:2100]\n",
        "#%%\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3242, 128, 128)\n",
            "(3242, 128, 128, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bs9X_kwkHM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assigning Labels\n",
        "\n",
        "# Define the number of classes\n",
        "num_classes = 3\n",
        "\n",
        "names = ['AMD','DME','NORMAL']\n",
        "\n",
        "# convert class labels to on-hot encoding\n",
        "Y = np_utils.to_categorical(labels, num_classes)\n",
        "x= img_data\n",
        "\n",
        "#Shuffle the dataset with random state=2\n",
        "x,y = shuffle(x, Y)\n",
        "# Split the dataset with 20% testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaJqmUp70-OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Defining the model \n",
        "# # Feel free to use CNNs/Dense Networks\n",
        "\n",
        "# model = Sequential()\n",
        "\n",
        "# model.add(Convolution2D(filters= 8, kernel_size=2, padding='same', activation='relu', input_shape=(128,128,1)))\n",
        "# #model.add(Conv2D(10, kernel_size=3, activation= 'relu', input_shape=train_set_x.shape[1:]  ))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# model.add(Convolution2D(filters= 16, kernel_size=2, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# model.add(Convolution2D(filters= 32, kernel_size=2, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# model.add(Convolution2D(filters= 64, kernel_size=2, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# model.add(Convolution2D(filters= 128, kernel_size=2, padding='same', activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(256, activation='relu'))\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dense(64, activation='relu'))\n",
        "# model.add(Dense(32, activation='relu'))\n",
        "# model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "# # Viewing model_configuration\n",
        "# #model.get_config()\n",
        "# #model.layers[0].get_config()\n",
        "# #model.layers[0].input_shape\n",
        "# #model.layers[0].output_shape\n",
        "# #model.layers[0].output\n",
        "# #model.layers[0].get_weights()\n",
        "# model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3ZVPVYl7XzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4def68b1-a015-400a-e4b5-e84df4122b2f"
      },
      "source": [
        "# Defining the model \n",
        "# Feel free to use CNNs/Dense Networks\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(filters= 8, kernel_size=2, padding='same', activation='relu'))\n",
        "#model.add(Conv2D(10, kernel_size=3, activation= 'relu', input_shape=train_set_x.shape[1:]  ))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(filters= 16, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(filters= 32, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(filters= 64, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Convolution2D(filters= 128, kernel_size=2, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "# Viewing model_configuration\n",
        "#model.get_config()\n",
        "#model.layers[0].get_config()\n",
        "#model.layers[0].input_shape\n",
        "#model.layers[0].output_shape\n",
        "#model.layers[0].output\n",
        "#model.layers[0].get_weights()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "menxB2M91AGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1359df05-48ab-4072-8f93-2e24feed5321"
      },
      "source": [
        "learning_rate = 0.001\n",
        "opt = adam(lr=learning_rate)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBhECEl1J6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3befed07-88ce-450b-8e97-2a0fe671dc4f"
      },
      "source": [
        "# Train and fit wit appropiate batch size, epochs, verbose = 1 and validation set\n",
        "hist = model.fit(X_train, y_train, epochs= 200, batch_size=20, verbose=True, validation_split=0.2)\n",
        "\n",
        "# model saving \n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2074 samples, validate on 519 samples\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2074/2074 [==============================] - 15s 7ms/step - loss: 0.5201 - acc: 0.7363 - val_loss: 0.4178 - val_acc: 0.7848\n",
            "Epoch 2/200\n",
            "2074/2074 [==============================] - 1s 443us/step - loss: 0.3555 - acc: 0.8356 - val_loss: 0.3524 - val_acc: 0.8349\n",
            "Epoch 3/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 0.2422 - acc: 0.8934 - val_loss: 0.2194 - val_acc: 0.9062\n",
            "Epoch 4/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 0.1281 - acc: 0.9495 - val_loss: 0.2167 - val_acc: 0.9216\n",
            "Epoch 5/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 0.0798 - acc: 0.9706 - val_loss: 0.0448 - val_acc: 0.9852\n",
            "Epoch 6/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 0.0400 - acc: 0.9859 - val_loss: 0.0232 - val_acc: 0.9917\n",
            "Epoch 7/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 0.0372 - acc: 0.9863 - val_loss: 0.0251 - val_acc: 0.9917\n",
            "Epoch 8/200\n",
            "2074/2074 [==============================] - 1s 402us/step - loss: 0.0195 - acc: 0.9949 - val_loss: 0.0498 - val_acc: 0.9820\n",
            "Epoch 9/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 0.0154 - acc: 0.9945 - val_loss: 0.0724 - val_acc: 0.9769\n",
            "Epoch 10/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.0249 - val_acc: 0.9910\n",
            "Epoch 11/200\n",
            "2074/2074 [==============================] - 1s 418us/step - loss: 0.0113 - acc: 0.9947 - val_loss: 0.0146 - val_acc: 0.9961\n",
            "Epoch 12/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 0.0289 - acc: 0.9883 - val_loss: 0.0323 - val_acc: 0.9917\n",
            "Epoch 13/200\n",
            "2074/2074 [==============================] - 1s 422us/step - loss: 0.0079 - acc: 0.9974 - val_loss: 0.0275 - val_acc: 0.9910\n",
            "Epoch 14/200\n",
            "2074/2074 [==============================] - 1s 417us/step - loss: 0.0107 - acc: 0.9960 - val_loss: 0.0110 - val_acc: 0.9949\n",
            "Epoch 15/200\n",
            "2074/2074 [==============================] - 1s 424us/step - loss: 3.9551e-04 - acc: 1.0000 - val_loss: 0.0401 - val_acc: 0.9897\n",
            "Epoch 16/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 0.0289 - acc: 0.9920 - val_loss: 0.0386 - val_acc: 0.9859\n",
            "Epoch 17/200\n",
            "2074/2074 [==============================] - 1s 418us/step - loss: 0.0044 - acc: 0.9984 - val_loss: 0.0333 - val_acc: 0.9923\n",
            "Epoch 18/200\n",
            "2074/2074 [==============================] - 1s 422us/step - loss: 0.0241 - acc: 0.9923 - val_loss: 0.0279 - val_acc: 0.9929\n",
            "Epoch 19/200\n",
            "2074/2074 [==============================] - 1s 419us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0203 - val_acc: 0.9974\n",
            "Epoch 20/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.1894e-04 - acc: 1.0000 - val_loss: 0.0168 - val_acc: 0.9961\n",
            "Epoch 21/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 4.7414e-05 - acc: 1.0000 - val_loss: 0.0188 - val_acc: 0.9974\n",
            "Epoch 22/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 2.5712e-05 - acc: 1.0000 - val_loss: 0.0202 - val_acc: 0.9974\n",
            "Epoch 23/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.8888e-05 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 0.9974\n",
            "Epoch 24/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.4553e-05 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 0.9974\n",
            "Epoch 25/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.1450e-05 - acc: 1.0000 - val_loss: 0.0221 - val_acc: 0.9974\n",
            "Epoch 26/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 9.1831e-06 - acc: 1.0000 - val_loss: 0.0224 - val_acc: 0.9974\n",
            "Epoch 27/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 7.3982e-06 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 0.9974\n",
            "Epoch 28/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 6.0001e-06 - acc: 1.0000 - val_loss: 0.0235 - val_acc: 0.9974\n",
            "Epoch 29/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 4.9799e-06 - acc: 1.0000 - val_loss: 0.0240 - val_acc: 0.9974\n",
            "Epoch 30/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 4.2049e-06 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 0.9974\n",
            "Epoch 31/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 3.5276e-06 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 0.9974\n",
            "Epoch 32/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 3.0271e-06 - acc: 1.0000 - val_loss: 0.0252 - val_acc: 0.9974\n",
            "Epoch 33/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 2.6091e-06 - acc: 1.0000 - val_loss: 0.0255 - val_acc: 0.9974\n",
            "Epoch 34/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 2.2717e-06 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 0.9974\n",
            "Epoch 35/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 2.0051e-06 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 0.9974\n",
            "Epoch 36/200\n",
            "2074/2074 [==============================] - 1s 441us/step - loss: 1.7633e-06 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 0.9974\n",
            "Epoch 37/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 1.5691e-06 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 0.9974\n",
            "Epoch 38/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.4071e-06 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 0.9974\n",
            "Epoch 39/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.2564e-06 - acc: 1.0000 - val_loss: 0.0264 - val_acc: 0.9974\n",
            "Epoch 40/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 1.1411e-06 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 0.9974\n",
            "Epoch 41/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.0320e-06 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 0.9974\n",
            "Epoch 42/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 9.3987e-07 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 0.9974\n",
            "Epoch 43/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 8.5102e-07 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 0.9974\n",
            "Epoch 44/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 7.7742e-07 - acc: 1.0000 - val_loss: 0.0268 - val_acc: 0.9974\n",
            "Epoch 45/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 7.0409e-07 - acc: 1.0000 - val_loss: 0.0269 - val_acc: 0.9974\n",
            "Epoch 46/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 6.4763e-07 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 0.9974\n",
            "Epoch 47/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 5.9930e-07 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9974\n",
            "Epoch 48/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 5.4897e-07 - acc: 1.0000 - val_loss: 0.0272 - val_acc: 0.9974\n",
            "Epoch 49/200\n",
            "2074/2074 [==============================] - 1s 417us/step - loss: 5.1047e-07 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 0.9974\n",
            "Epoch 50/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 4.7418e-07 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 0.9974\n",
            "Epoch 51/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 4.4079e-07 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9974\n",
            "Epoch 52/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 4.1118e-07 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9974\n",
            "Epoch 53/200\n",
            "2074/2074 [==============================] - 1s 425us/step - loss: 3.8813e-07 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 0.9974\n",
            "Epoch 54/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 3.6544e-07 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 0.9974\n",
            "Epoch 55/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 3.4363e-07 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 0.9974\n",
            "Epoch 56/200\n",
            "2074/2074 [==============================] - 1s 417us/step - loss: 3.2320e-07 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9974\n",
            "Epoch 57/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 3.0694e-07 - acc: 1.0000 - val_loss: 0.0278 - val_acc: 0.9974\n",
            "Epoch 58/200\n",
            "2074/2074 [==============================] - 1s 454us/step - loss: 2.9279e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 59/200\n",
            "2074/2074 [==============================] - 1s 417us/step - loss: 2.7567e-07 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9974\n",
            "Epoch 60/200\n",
            "2074/2074 [==============================] - 1s 449us/step - loss: 2.6316e-07 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 0.9974\n",
            "Epoch 61/200\n",
            "2074/2074 [==============================] - 1s 419us/step - loss: 2.5170e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 62/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 2.4070e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 63/200\n",
            "2074/2074 [==============================] - 1s 428us/step - loss: 2.3142e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 64/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 2.2219e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 65/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 2.1457e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 66/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 2.0667e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 67/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.9834e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 68/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.9273e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 69/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.8618e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 70/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.8048e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 71/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.7581e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 72/200\n",
            "2074/2074 [==============================] - 1s 434us/step - loss: 1.7234e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 73/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 1.6851e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 74/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.6398e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 75/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.5988e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 76/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.5720e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 77/200\n",
            "2074/2074 [==============================] - 1s 402us/step - loss: 1.5396e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 78/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.5090e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 79/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.4772e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 80/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.4553e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 81/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.4327e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 82/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.4080e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 83/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.3841e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 84/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.3651e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 85/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.3465e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 86/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.3362e-07 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 0.9974\n",
            "Epoch 87/200\n",
            "2074/2074 [==============================] - 1s 442us/step - loss: 1.3101e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 88/200\n",
            "2074/2074 [==============================] - 1s 429us/step - loss: 1.2997e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 89/200\n",
            "2074/2074 [==============================] - 1s 425us/step - loss: 1.2886e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 90/200\n",
            "2074/2074 [==============================] - 1s 426us/step - loss: 1.2691e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 91/200\n",
            "2074/2074 [==============================] - 1s 422us/step - loss: 1.2577e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 92/200\n",
            "2074/2074 [==============================] - 1s 417us/step - loss: 1.2487e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 93/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.2342e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 94/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.2259e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 95/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.2147e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 96/200\n",
            "2074/2074 [==============================] - 1s 424us/step - loss: 1.2084e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 97/200\n",
            "2074/2074 [==============================] - 1s 422us/step - loss: 1.2002e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 98/200\n",
            "2074/2074 [==============================] - 1s 430us/step - loss: 1.1893e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 99/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.1843e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 100/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.1755e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 101/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.1667e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 102/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.1603e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 103/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.1569e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 104/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.1492e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 105/200\n",
            "2074/2074 [==============================] - 1s 418us/step - loss: 1.1448e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 106/200\n",
            "2074/2074 [==============================] - 1s 440us/step - loss: 1.1381e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 107/200\n",
            "2074/2074 [==============================] - 1s 418us/step - loss: 1.1337e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 108/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.1305e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 109/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.1263e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 110/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.1230e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 111/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.1185e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 112/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.1151e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 113/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.1123e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 114/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.1089e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 115/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.1058e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 116/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.1032e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 117/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.1000e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 118/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.0983e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 119/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.0962e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 120/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 1.0933e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 121/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.0920e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 122/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.0900e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 123/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.0889e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 124/200\n",
            "2074/2074 [==============================] - 1s 395us/step - loss: 1.0875e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 125/200\n",
            "2074/2074 [==============================] - 1s 419us/step - loss: 1.0863e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 126/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.0839e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 127/200\n",
            "2074/2074 [==============================] - 1s 423us/step - loss: 1.0825e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 128/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0819e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 129/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.0806e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 130/200\n",
            "2074/2074 [==============================] - 1s 440us/step - loss: 1.0798e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 131/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.0780e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 132/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.0777e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 133/200\n",
            "2074/2074 [==============================] - 1s 419us/step - loss: 1.0770e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 134/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.0769e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 135/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.0745e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 136/200\n",
            "2074/2074 [==============================] - 1s 396us/step - loss: 1.0739e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 137/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.0732e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 138/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.0729e-07 - acc: 1.0000 - val_loss: 0.0280 - val_acc: 0.9974\n",
            "Epoch 139/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.0719e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 140/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.0716e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 141/200\n",
            "2074/2074 [==============================] - 1s 432us/step - loss: 1.0710e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 142/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.0709e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 143/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0701e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 144/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.0699e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 145/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.0690e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 146/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0687e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 147/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.0682e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 148/200\n",
            "2074/2074 [==============================] - 1s 400us/step - loss: 1.0677e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 149/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.0673e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 150/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0671e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 151/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 1.0668e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 152/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.0665e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 153/200\n",
            "2074/2074 [==============================] - 1s 420us/step - loss: 1.0661e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 154/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.0663e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 155/200\n",
            "2074/2074 [==============================] - 1s 419us/step - loss: 1.0659e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 156/200\n",
            "2074/2074 [==============================] - 1s 428us/step - loss: 1.0658e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 157/200\n",
            "2074/2074 [==============================] - 1s 418us/step - loss: 1.0657e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 158/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.0652e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 159/200\n",
            "2074/2074 [==============================] - 1s 421us/step - loss: 1.0651e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 160/200\n",
            "2074/2074 [==============================] - 1s 402us/step - loss: 1.0651e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 161/200\n",
            "2074/2074 [==============================] - 1s 399us/step - loss: 1.0649e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 162/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.0649e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 163/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.0648e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 164/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.0646e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 165/200\n",
            "2074/2074 [==============================] - 1s 411us/step - loss: 1.0645e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 166/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.0645e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 167/200\n",
            "2074/2074 [==============================] - 1s 397us/step - loss: 1.0645e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 168/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.0645e-07 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 0.9974\n",
            "Epoch 169/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.0642e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 170/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.0642e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 171/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.0642e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 172/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.0642e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 173/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 174/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 175/200\n",
            "2074/2074 [==============================] - 1s 414us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 176/200\n",
            "2074/2074 [==============================] - 1s 437us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 177/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 178/200\n",
            "2074/2074 [==============================] - 1s 407us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 179/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0282 - val_acc: 0.9974\n",
            "Epoch 180/200\n",
            "2074/2074 [==============================] - 1s 412us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 181/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 182/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 183/200\n",
            "2074/2074 [==============================] - 1s 408us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 184/200\n",
            "2074/2074 [==============================] - 1s 402us/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 185/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 186/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 187/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 188/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 189/200\n",
            "2074/2074 [==============================] - 1s 409us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9974\n",
            "Epoch 190/200\n",
            "2074/2074 [==============================] - 1s 406us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 191/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 192/200\n",
            "2074/2074 [==============================] - 1s 415us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 0.9974\n",
            "Epoch 193/200\n",
            "2074/2074 [==============================] - 1s 410us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 194/200\n",
            "2074/2074 [==============================] - 1s 416us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 0.9974\n",
            "Epoch 195/200\n",
            "2074/2074 [==============================] - 1s 404us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 196/200\n",
            "2074/2074 [==============================] - 1s 405us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 197/200\n",
            "2074/2074 [==============================] - 1s 425us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 198/200\n",
            "2074/2074 [==============================] - 1s 425us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 0.9974\n",
            "Epoch 199/200\n",
            "2074/2074 [==============================] - 1s 413us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Epoch 200/200\n",
            "2074/2074 [==============================] - 1s 437us/step - loss: 1.0640e-07 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 0.9974\n",
            "Saved model to disk\n",
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huW-s0FN4F_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fd5be88f-cc38-4b52-ce73-111d5fe707e3"
      },
      "source": [
        "# Evaluate the model w.r.t Test Loss and Test Accuracy\n",
        "score = model.evaluate(X_train, y_train)\n",
        "print('Test Loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2593/2593 [==============================] - 0s 128us/step\n",
            "Test Loss: 0.0056896836066546086\n",
            "Test accuracy: 0.9994857955797272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANeEUsoZ1L7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "outputId": "be535500-df6c-4d1c-a029-e680b8b200fa"
      },
      "source": [
        "# Predict model on Test Data\n",
        "\n",
        "Y_pred = model.predict_classes(X_test)\n",
        "print(Y_pred)\n",
        "\n",
        "# Printing the confusion matrix\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import itertools\n",
        "\n",
        "# Print the classes of the Prediction\n",
        "# y_pred = y_test\n",
        "# print(y_pred)\n",
        "\n",
        "target_names = ['AMD', 'DME', 'Normal']\n",
        "                                        \n",
        "# Plotting the confusion matrix\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"blue\" if cm[i, j] > thresh else \"blue\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "#Computation  confusion matrix\n",
        "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), Y_pred))\n",
        "\n",
        "np.set_printoptions(precision=10)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Plotting non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
        "                      title='Confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 1 1 2 2 2 2 2 2 2 2 2 1 1 2 1 1 2 2 2 2 2 2 2 2 0 1 2 1 1 0 0 0 2 2\n",
            " 1 0 1 0 2 2 2 0 2 0 0 2 1 1 2 1 2 2 2 2 0 1 1 1 0 2 1 0 0 1 1 2 1 2 0 2 2\n",
            " 2 0 2 1 1 1 1 1 0 0 0 2 2 2 0 1 2 2 1 2 1 1 2 0 2 0 1 2 2 2 0 2 1 2 2 1 0\n",
            " 1 1 1 1 1 1 1 1 2 2 2 2 1 1 2 1 1 1 1 1 2 1 2 0 2 1 1 2 2 1 2 2 2 2 2 2 2\n",
            " 0 2 0 0 2 2 2 1 2 1 2 2 1 1 1 1 1 2 0 1 0 0 2 1 0 2 2 2 2 0 1 2 1 0 1 1 2\n",
            " 1 1 0 0 2 2 1 1 2 2 2 1 1 2 1 2 2 1 2 2 1 1 0 2 2 1 0 2 1 2 2 0 1 2 1 1 2\n",
            " 2 1 2 0 2 2 2 0 1 2 0 0 0 1 0 2 0 2 1 2 2 0 2 2 1 2 2 0 0 2 2 0 2 0 2 0 2\n",
            " 0 2 0 2 2 2 1 0 1 1 0 2 0 2 0 1 1 2 1 0 1 2 2 1 2 2 0 2 2 2 1 0 2 2 1 2 0\n",
            " 0 1 1 0 0 1 2 2 2 1 1 1 2 0 2 2 0 1 1 2 0 0 2 1 2 0 0 1 2 1 1 2 2 0 2 0 1\n",
            " 1 1 1 2 2 1 0 1 1 1 2 0 2 2 2 0 2 0 1 0 1 1 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1\n",
            " 2 0 2 1 1 0 1 2 1 2 1 1 1 1 0 0 2 1 0 2 1 0 1 1 1 0 1 2 0 1 0 2 0 0 2 1 2\n",
            " 1 1 2 1 2 2 1 0 2 0 2 1 2 2 2 1 2 1 2 1 2 2 2 2 2 2 1 1 2 0 0 1 2 1 1 0 2\n",
            " 0 1 1 0 1 0 2 1 1 1 2 2 2 1 2 0 2 1 1 2 2 1 1 1 1 1 2 2 2 2 1 1 2 1 2 1 1\n",
            " 2 0 0 1 2 1 2 2 0 0 1 0 0 2 1 2 2 2 1 0 1 2 1 0 1 1 2 1 2 1 2 2 0 2 1 2 1\n",
            " 1 2 2 2 0 1 1 0 2 0 0 2 1 1 2 2 1 1 0 1 0 0 2 0 1 2 0 0 0 1 1 1 2 1 2 2 2\n",
            " 1 0 2 2 1 2 0 2 1 0 2 2 0 1 1 0 2 2 1 1 1 0 1 1 0 2 2 1 1 1 2 2 2 2 0 0 2\n",
            " 2 2 2 0 2 1 1 0 2 2 1 0 1 1 2 1 2 1 0 2 0 0 0 1 2 1 0 1 1 1 1 1 1 2 2 1 2\n",
            " 0 1 1 2 2 1 0 2 1 2 2 0 1 0 2 0 0 0 1 0]\n",
            "Confusion matrix, without normalization\n",
            "[[143   0   0]\n",
            " [  0 239   0]\n",
            " [  0   0 267]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEmCAYAAADfpHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnEzNzIrAydTMrAicTM3MiuD/A5EbeBfAMc7GAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I47A9OV2KCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}